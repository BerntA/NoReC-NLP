{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis on the NoReC dataset, using support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stian\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\stian\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Run these if you cannot import the nltk libs.\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv('data/data.csv', encoding='utf-8')\n",
    "# articles['content_split'] = articles.loc[:, 'content'].apply(lambda x: x.split())\n",
    "articles['sentiment'] = [(1 if rating > 3 else 0) for rating in articles.loc[:, 'rating']]\n",
    "articles['three_sentiment'] = [(1 if rating > 4 else 0 if rating < 3 else 2) for rating in articles.loc[:, 'rating']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43614 records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>cons</th>\n",
       "      <th>language</th>\n",
       "      <th>pros</th>\n",
       "      <th>rating</th>\n",
       "      <th>source</th>\n",
       "      <th>source-category</th>\n",
       "      <th>source-tags</th>\n",
       "      <th>split</th>\n",
       "      <th>tags</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>three_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>screen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>p3</td>\n",
       "      <td>tv</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>['tv']</td>\n",
       "      <td>rom s topp inn tvdram akkurat andr sist sesong...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>screen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>p3</td>\n",
       "      <td>tv</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>['tv']</td>\n",
       "      <td>twin peaks definitiv gold box edition gull twi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>screen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>p3</td>\n",
       "      <td>tv</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>['tv']</td>\n",
       "      <td>the wir sesong the wir gjør avheng god måt nes...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>screen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>p3</td>\n",
       "      <td>tv</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>['tv']</td>\n",
       "      <td>mad sesong stil underhold sofistiker tvseri ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>screen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>p3</td>\n",
       "      <td>film</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>['movie']</td>\n",
       "      <td>mad sesong tvunderholdning høyest kvalit først...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category cons language pros  rating source source-category source-tags  \\\n",
       "0   screen  NaN       nb  NaN       6     p3              tv          []   \n",
       "1   screen  NaN       nb  NaN       6     p3              tv          []   \n",
       "2   screen  NaN       nb  NaN       6     p3              tv          []   \n",
       "3   screen  NaN       nb  NaN       5     p3              tv          []   \n",
       "4   screen  NaN       nb  NaN       5     p3            film          []   \n",
       "\n",
       "   split       tags                                            content  \\\n",
       "0  train     ['tv']  rom s topp inn tvdram akkurat andr sist sesong...   \n",
       "1  train     ['tv']  twin peaks definitiv gold box edition gull twi...   \n",
       "2  train     ['tv']  the wir sesong the wir gjør avheng god måt nes...   \n",
       "3  train     ['tv']  mad sesong stil underhold sofistiker tvseri ma...   \n",
       "4  train  ['movie']  mad sesong tvunderholdning høyest kvalit først...   \n",
       "\n",
       "   sentiment  three_sentiment  \n",
       "0          1                1  \n",
       "1          1                1  \n",
       "2          1                1  \n",
       "3          1                1  \n",
       "4          1                1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(articles), 'records')\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train & test\n",
    "Using Support Vector Classifier + Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    32910\n",
       "0    10704\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.sentiment.value_counts() # 1 = positive, 0 = negative. As we can see, much larger amount of pos. reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "review_content_tf = cv.fit_transform(articles.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43614, 368212)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_content_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_cv = TfidfVectorizer()\n",
    "review_content_tf = tf_idf_cv.fit_transform(articles.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma-function = \"auto\", kernel = \"rbf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 75.285%.\n",
      "Training time: 1185.5434720516205 \n",
      "Testing time: 358.3650348186493\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf, articles.sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_rbf_auto = SVC(gamma = \"auto\")\n",
    "svc_rbf_auto.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time() \n",
    "y_pred = svc_rbf_auto.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma-function = \"scale\", kernel = \"rbf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 76.217%.\n",
      "Training time: 1089.7594327926636 \n",
      "Testing time: 371.46812915802\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf, articles.sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_rbf_scale = SVC(gamma = \"scale\", kernel = \"rbf\")\n",
    "svc_rbf_scale.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_rbf_scale.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma-function = \"auto\", kernel = \"linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 81.735%.\n",
      "Training time: 6771.545135259628 \n",
      "Testing time: 240.38828945159912\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf, articles.sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_linear = SVC(gamma = \"auto\", kernel = \"linear\")\n",
    "svc_linear.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_linear.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorizer with all kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_cv = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
    "review_content_tf_idf = tf_idf_cv.fit_transform(articles.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43614, 65793)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_content_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 84.723%.\n",
      "Training time: 1721.595618724823 \n",
      "Testing time: 285.8620102405548\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf_idf, articles.sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_linear_tf_idf = SVC(gamma = \"auto\", kernel = \"linear\")\n",
    "svc_linear_tf_idf.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_linear_tf_idf.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54141475, 0.45858525],\n",
       "       [0.04929938, 0.95070062]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(testY, y_pred, normalize=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma-function = \"scale\", kernel = \"rbf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 75.231%.\n",
      "Training time: 1217.4950003623962 \n",
      "Testing time: 339.579998254776\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf_idf, articles.sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_rbf_scale = SVC(gamma = \"scale\", kernel = \"rbf\")\n",
    "svc_rbf_scale.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_rbf_scale.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma-function = \"auto\", kernel = \"poly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 75.438%.\n",
      "Training time: 695.379992723465 \n",
      "Testing time: 289.2979452610016\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf_idf, articles.sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_poly_auto = SVC(gamma = \"auto\", kernel = \"poly\")\n",
    "svc_poly_auto.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_poly_auto.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma-function = \"scale\", kernel = \"poly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 75.827%.\n",
      "Training time: 699.3269975185394 \n",
      "Testing time: 290.5860013961792\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf_idf, articles.sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_poly_scale = SVC(gamma = \"scale\", kernel = \"poly\")\n",
    "svc_poly_scale.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_poly_scale.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma-function = \"auto\", kernel = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 75.529%.\n",
      "Training time: 714.6609628200531 \n",
      "Testing time: 296.1706907749176\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf_idf, articles.sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_sigmoid_auto = SVC(gamma = \"auto\", kernel = \"sigmoid\")\n",
    "svc_sigmoid_auto.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_sigmoid_auto.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma-function = \"scale\", kernel = \"sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 75.430%.\n",
      "Training time: 1152.581962108612 \n",
      "Testing time: 314.5415563583374\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf_idf, articles.sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_sigmoid_scale = SVC(gamma = \"scale\", kernel = \"sigmoid\")\n",
    "svc_sigmoid_scale.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_sigmoid_scale.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_cv = TfidfVectorizer(min_df = 5, max_df = 0.8, sublinear_tf = True, use_idf = True)\n",
    "review_content_tf_idf = tf_idf_cv.fit_transform(articles.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43614, 65792)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_content_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 71.257%.\n",
      "Training time: 2645.7904319763184 \n",
      "Testing time: 472.92552042007446\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf_idf, articles.three_sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_linear_multiclass = SVC(gamma = \"auto\", kernel = \"linear\", decision_function_shape='ovo')\n",
    "svc_linear_multiclass.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_linear_multiclass.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 13085 points, our accuracy were 71.716%.\n",
      "Training time: 2647.3154096603394 \n",
      "Testing time: 477.9991874694824\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(review_content_tf_idf, articles.three_sentiment, test_size=0.3)\n",
    "\n",
    "t0 = time.time()\n",
    "svc_linear_multiclass = SVC(gamma = \"auto\", kernel = \"linear\", decision_function_shape='ovr')\n",
    "svc_linear_multiclass.fit(trainX, trainY)\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = svc_linear_multiclass.predict(testX)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Out of\", len(testY), 'points, our accuracy were {:.03f}%.'.format(100*(1-(y_pred != testY).sum()/len(testY))))\n",
    "print(\"Training time:\", t1 - t0, \"\\nTesting time:\", t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [svc_rbf_auto, svc_rbf_scale, svc_linear, svc_linear_tf_idf, svc_poly_auto, svc_poly_scale, svc_sigmoid_auto, svc_sigmoid_scale, svc_linear_multiclass]\n",
    "model_names = [\"svc_rbf_auto\", \"svc_rbf_scale\", \"svc_linear\", \"svc_linear_tf_idf\", \"svc_poly_auto_tf_idf\", \"svc_poly_scale_tf_idf\", \"svc_sigmoid_auto_tf_idf\", \"svc_sigmoid_scale_tf_idf\", \"svc_linear_multiclass_tf_idf\"]\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    pickle_file = open(\"data/models/\" + model_names[idx], \"wb\")\n",
    "    pickle.dump(model, pickle_file)\n",
    "    pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stemmer = SnowballStemmer(\"norwegian\", ignore_stopwords=True)\n",
    "excludedStopWords = set(['ikkje', 'ikke', 'inkje'])\n",
    "stopWords = set([word for word in set(stopwords.words('norwegian')) if word not in excludedStopWords])\n",
    "FEATURES = tf_idf_cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictReview(text, classifier):\n",
    "    \"\"\"\n",
    "    Quickly classify test reviews using our model.\n",
    "    Do some quick preprocessing, (copy pasted from preprocessing.ipynb....)\n",
    "    \"\"\"\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r'[^a-zA-ZæøåÆØÅéäöÄÖ -!?]+', '', text) # Remove any symbols\n",
    "    text = re.sub(r'\\s\\s+', ' ', text) # Remove consequent whitespace    \n",
    "    text = [word_stemmer.stem(word) for word in word_tokenize(text) if word not in stopWords]\n",
    "    \n",
    "    word_count = {w:0 for w in FEATURES}\n",
    "    for w in text:\n",
    "        if w in word_count:\n",
    "            word_count[w] += 1\n",
    "    \n",
    "    text_tf = np.array([[v for _,v in word_count.items()]])\n",
    "    res = classifier.predict(text_tf)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = [\n",
    "    'dårlig ikke elendig søppel!',\n",
    "    'ikke gale!',\n",
    "    'den trenger litt finpuss men ellers helt fin',\n",
    "    'den falt ikke i min smak, håper på at sesong 3 blir bedre',\n",
    "    'det kan ikke bli værre musikk en dette her',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[predictReview(review, svc) for review in test_reviews] "
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('base': conda)",
   "language": "python",
   "name": "python37164bitbasecondacfb15ef7abb74187aaec3c70effd709f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
